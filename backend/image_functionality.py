# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7tMFMvqR_jVk_eWorMyCBHP0P4n1QeC
"""
import cv2
from deepface import DeepFace
import face_recognition
from PIL import Image as im

#VALIDATION PRINTS
# print(cv2.__version__)
# print(DeepFace.__version__)
# print(face_recognition.__version__)

RTMP_URL = "rtmp://162.243.166.134:1935/live/test" #I think extra configs needed in nginx.conf

mapping = []

cap = cv2.VideoCapture(RTMP_URL)

print("got caputre")

def play_tone(face_encoding):
    print("PLAY TONE: send message to client made up of the compressed face_encoding so they can play it")



def check_if_in_mapping(face_encoding):

    for key in mapping:
        result = face_recognition.compare_faces([face_encoding], key)

        if result[0]:
            return True

    return False

def caputure_from_video():
    name = ""#Where is name coming from??
    process_this_frame = 0
    frame_skips = 100 #Use (1/frame_skips) frames; ex) 1/3 skips 2 of 3 frames
    frame_count = 0
    while cap.isOpened():  # Untill end of file/error occured
      ret, frame = cap.read()

      if ret and (frame_count % frame_skips == 0):
        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
        # rgb_small_frame = small_frame[:, :, ::-1]

        face_locations = face_recognition.face_locations(frame)
        if face_locations:
        # Compute the facial encodings for the faces detected
            face_encodings = face_recognition.face_encodings(frame, known_face_locations=face_locations)
            
            # You can now proceed with the face encodings (e.g., comparing or storing them)
            print("Face encodings:", len(face_encodings))
        else:
            print("No face locations detected")
            continue

        for face_encoding in face_encodings:
          if not check_if_in_mapping(face_encoding):
            mapping.append(face_encoding)
          play_tone(face_encoding)


        #Emotion Detection With DeepFace
        data = im.fromarray(frame)
        data.save("image_for_deepface.jpg")

        try:
            objs = DeepFace.analyze(
            img_path = "image_for_deepface.jpg",
            actions = ['emotion'],
            )

            print("DeepFace Analysis", objs)
        except:
           print("Deepface failed")

      frame_count +=1

    for face_encoding in face_encodings:
        if check_if_in_mapping(face_encoding):
            play_tone(face_encoding)
        else:
            # mapping[face_encoding] = name
            print("Skip")
            
    # process_this_frame = not process_this_frame
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
   caputure_from_video()